{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2026-01-09T17:30:55.262962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# XGBOOST + OPTUNA + FEATURE SELECTION\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import (roc_auc_score, classification_report, confusion_matrix,\n",
    "                             precision_recall_curve, f1_score, average_precision_score)\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ XGBOOST + OPTUNA + FEATURE SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ==========================================\n",
    "# 1. CARGA DE DATOS\n",
    "# ==========================================\n",
    "print(\"\\nüìÇ Cargando datos...\")\n",
    "df = pd.read_parquet(\"../data/interim/train_final_advanced_features.parquet\")\n",
    "\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "cols_to_drop = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Dataset: {df.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. PREPARAR FEATURES\n",
    "# ==========================================\n",
    "X = df.drop(['TARGET', 'SK_ID_CURR'], axis=1, errors='ignore')\n",
    "y = df['TARGET']\n",
    "\n",
    "# Encoding\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in cat_cols:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
    "\n",
    "# ==========================================\n",
    "# 3. FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "print(\"\\nüîß Feature Engineering...\")\n",
    "\n",
    "\n",
    "def complete_features(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    ext1 = df.get('EXT_SOURCE_1', pd.Series([0.5] * len(df))).fillna(0.5).replace(0, 0.5)\n",
    "    ext2 = df.get('EXT_SOURCE_2', pd.Series([0.5] * len(df))).fillna(0.5).replace(0, 0.5)\n",
    "    ext3 = df.get('EXT_SOURCE_3', pd.Series([0.5] * len(df))).fillna(0.5).replace(0, 0.5)\n",
    "\n",
    "    ext_df = pd.concat([ext1, ext2, ext3], axis=1)\n",
    "    df['EXT_mean'] = ext_df.mean(axis=1)\n",
    "    df['EXT_std'] = ext_df.std(axis=1)\n",
    "    df['EXT_min'] = ext_df.min(axis=1)\n",
    "    df['EXT_max'] = ext_df.max(axis=1)\n",
    "    df['EXT_sum'] = ext1 + ext2 + ext3\n",
    "    df['EXT_prod'] = ext1 * ext2 * ext3\n",
    "\n",
    "    df['EXT_w_01_06_03'] = ext1 * 0.1 + ext2 * 0.6 + ext3 * 0.3\n",
    "    df['EXT_1x2'] = ext1 * ext2\n",
    "    df['EXT_2x3'] = ext2 * ext3\n",
    "    df['EXT_1x3'] = ext1 * ext3\n",
    "\n",
    "    df['EXT_2_pow2'] = ext2 ** 2\n",
    "    df['EXT_2_log'] = np.log1p(ext2)\n",
    "    df['EXT_harmonic'] = 3 / (1 / (ext1 + 0.01) + 1 / (ext2 + 0.01) + 1 / (ext3 + 0.01))\n",
    "    df['EXT_geometric'] = (ext1 * ext2 * ext3) ** (1 / 3)\n",
    "\n",
    "    if 'DAYS_BIRTH' in df.columns:\n",
    "        df['age'] = -df['DAYS_BIRTH'] / 365.25\n",
    "        df['age_sq'] = df['age'] ** 2\n",
    "\n",
    "    if 'DAYS_EMPLOYED' in df.columns:\n",
    "        days_emp = df['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "        df['emp_years'] = (-days_emp / 365.25).clip(lower=0)\n",
    "        df['is_unemployed'] = (df['DAYS_EMPLOYED'] == 365243).astype(int)\n",
    "\n",
    "    if 'age' in df.columns and 'emp_years' in df.columns:\n",
    "        df['emp_ratio'] = df['emp_years'].fillna(0) / (df['age'] + 0.01)\n",
    "\n",
    "    income = df.get('AMT_INCOME_TOTAL', pd.Series([1] * len(df))).replace(0, np.nan).fillna(1)\n",
    "    credit = df.get('AMT_CREDIT', pd.Series([1] * len(df))).replace(0, np.nan).fillna(1)\n",
    "    annuity = df.get('AMT_ANNUITY', pd.Series([1] * len(df))).replace(0, np.nan).fillna(1)\n",
    "\n",
    "    df['cr_inc'] = (credit / income).clip(upper=50)\n",
    "    df['an_inc'] = (annuity / income).clip(upper=5)\n",
    "    df['cr_an'] = (credit / annuity).clip(upper=100)\n",
    "\n",
    "    if 'age' in df.columns:\n",
    "        df['EXT2_age'] = ext2 * df['age']\n",
    "\n",
    "    df['EXT2_d_crInc'] = ext2 / (df['cr_inc'] + 0.01)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "X = complete_features(X)\n",
    "X = X.loc[:, ~X.columns.duplicated()]\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].isna().any():\n",
    "        median_val = X[col].median()\n",
    "        X[col] = X[col].fillna(median_val if pd.notna(median_val) else 0)\n",
    "\n",
    "print(f\"Features despu√©s de engineering: {X.shape[1]}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3.5 FEATURE SELECTION ‚¨ÖÔ∏è AQU√ç VA\n",
    "# ==========================================\n",
    "print(\"\\nüéØ Feature Selection...\")\n",
    "\n",
    "selector_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    device='cuda',\n",
    "    tree_method='hist',\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Muestra balanceada para selecci√≥n r√°pida\n",
    "sample_idx = y.sample(frac=0.3, random_state=42).index\n",
    "X_sample = X.loc[sample_idx]\n",
    "y_sample = y.loc[sample_idx]\n",
    "\n",
    "sample_df = pd.concat([X_sample, y_sample], axis=1)\n",
    "c0 = sample_df[sample_df['TARGET'] == 0]\n",
    "c1 = sample_df[sample_df['TARGET'] == 1]\n",
    "c0_sub = resample(c0, replace=False, n_samples=len(c1), random_state=42)\n",
    "sample_balanced = pd.concat([c0_sub, c1])\n",
    "\n",
    "X_sel = sample_balanced.drop('TARGET', axis=1)\n",
    "y_sel = sample_balanced['TARGET']\n",
    "\n",
    "selector_model.fit(X_sel, y_sel)\n",
    "\n",
    "fi_selector = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': selector_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# ‚ö° AJUSTAR ESTE N√öMERO: 50, 100, 150\n",
    "N_FEATURES = 100\n",
    "\n",
    "top_features = fi_selector.head(N_FEATURES)['feature'].tolist()\n",
    "\n",
    "print(f\"Features originales: {X.shape[1]}\")\n",
    "print(f\"Features seleccionadas: {N_FEATURES}\")\n",
    "print(f\"\\nTop 10 features:\")\n",
    "for _, row in fi_selector.head(10).iterrows():\n",
    "    print(f\"  {row['feature']:<40} {row['importance']:.6f}\")\n",
    "\n",
    "# Aplicar selecci√≥n\n",
    "X = X[top_features]\n",
    "\n",
    "# ==========================================\n",
    "# 4. SPLIT (ANTES DE BALANCEAR)\n",
    "# ==========================================\n",
    "print(\"\\nüìä Split estratificado...\")\n",
    "\n",
    "X_train_raw, X_test, y_train_raw, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train_raw.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. BALANCEO UNDERSAMPLING 1:1\n",
    "# ==========================================\n",
    "print(\"\\n‚öñÔ∏è Balanceando...\")\n",
    "\n",
    "train_df = pd.concat([X_train_raw, y_train_raw], axis=1)\n",
    "class_0 = train_df[train_df['TARGET'] == 0]\n",
    "class_1 = train_df[train_df['TARGET'] == 1]\n",
    "\n",
    "class_0_sub = resample(class_0, replace=False, n_samples=len(class_1), random_state=42)\n",
    "train_balanced = pd.concat([class_0_sub, class_1]).sample(frac=1, random_state=42)\n",
    "\n",
    "X_train_full = train_balanced.drop('TARGET', axis=1)\n",
    "y_train_full = train_balanced['TARGET']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. OPTUNA\n",
    "# ==========================================\n",
    "print(\"\\nüîç Optimizando con Optuna...\")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': 3000,\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 20, 150),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.6),\n",
    "        'gamma': trial.suggest_float('gamma', 0.5, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10, log=True),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 5),\n",
    "        'max_delta_step': trial.suggest_int('max_delta_step', 0, 5),\n",
    "        'device': 'cuda',\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'auc',\n",
    "        'early_stopping_rounds': 150,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True)\n",
    "\n",
    "    pred_val = model.predict_proba(X_val)[:, 1]\n",
    "    pred_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    trial.set_user_attr('auc_test', roc_auc_score(y_test, pred_test))\n",
    "\n",
    "    return roc_auc_score(y_val, pred_val)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=200, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Mejor AUC Val: {study.best_value:.4f}\")\n",
    "print(f\"‚úÖ Mejor AUC Test: {study.best_trial.user_attrs['auc_test']:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 7. MODELO FINAL\n",
    "# ==========================================\n",
    "print(\"\\nüéØ Entrenando modelo final...\")\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'n_estimators': 5000,\n",
    "    'device': 'cuda',\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42,\n",
    "    'eval_metric': 'auc',\n",
    "    'early_stopping_rounds': 300,\n",
    "    'verbosity': 0\n",
    "})\n",
    "\n",
    "X_tr, X_vl, y_tr, y_vl = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.15, random_state=42, stratify=y_train_full\n",
    ")\n",
    "\n",
    "final_model = XGBClassifier(**best_params)\n",
    "final_model.fit(X_tr, y_tr, eval_set=[(X_vl, y_vl)], verbose=True)\n",
    "\n",
    "pred_test = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ==========================================\n",
    "# 8. M√âTRICAS FINALES\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä RESULTADOS FINALES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "auc = roc_auc_score(y_test, pred_test)\n",
    "ap = average_precision_score(y_test, pred_test)\n",
    "\n",
    "print(f\"\\nüìà ROC-AUC: {auc:.4f}\")\n",
    "print(f\"üìà Average Precision: {ap:.4f}\")\n",
    "\n",
    "# Threshold √≥ptimo\n",
    "prec, rec, thresh = precision_recall_curve(y_test, pred_test)\n",
    "f1_scores = 2 * prec * rec / (prec + rec + 1e-10)\n",
    "opt_thresh = thresh[np.argmax(f1_scores)]\n",
    "\n",
    "y_pred = (pred_test >= opt_thresh).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nüìä Con threshold {opt_thresh:.3f}:\")\n",
    "print(f\"  Recall: {cm[1, 1] / (cm[1, 0] + cm[1, 1]):.2%}\")\n",
    "print(f\"  Precision: {cm[1, 1] / (cm[0, 1] + cm[1, 1]):.2%}\")\n",
    "print(f\"  Defaults detectados: {cm[1, 1]:,} / {cm[1, 0] + cm[1, 1]:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ],
   "id": "9eb20a10ae80fe8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ XGBOOST + OPTUNA + FEATURE SELECTION\n",
      "============================================================\n",
      "\n",
      "üìÇ Cargando datos...\n",
      "Dataset: (307511, 218)\n",
      "\n",
      "üîß Feature Engineering...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d51044c47b2a34c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9b08c93515feb76b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T20:49:07.061674Z",
     "start_time": "2026-01-04T19:47:35.655219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# OPTUNA BLEND\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (roc_auc_score, classification_report, confusion_matrix,\n",
    "                             precision_recall_curve, f1_score, average_precision_score)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ MODELO FINAL V8 - OPTUNA OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==========================================\n",
    "# 1. CARGA Y PREPARACI√ìN\n",
    "# ==========================================\n",
    "print(\"\\nüìÇ Cargando datos...\")\n",
    "df = pd.read_parquet(\"../data/interim/train_final_advanced_features.parquet\")\n",
    "\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "cols_to_drop = [c for c in df.columns if c.endswith('_x') or c.endswith('_y')]\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "X = df.drop(['TARGET', 'SK_ID_CURR'], axis=1, errors='ignore')\n",
    "y = df['TARGET']\n",
    "\n",
    "# Encoding\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in cat_cols:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
    "\n",
    "# ==========================================\n",
    "# 2. FEATURE ENGINEERING COMPLETO\n",
    "# ==========================================\n",
    "print(\"\\nüîß Feature Engineering...\")\n",
    "\n",
    "def complete_features(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    ext1 = df.get('EXT_SOURCE_1', pd.Series([0.5]*len(df))).replace(0, 0.5)\n",
    "    ext2 = df.get('EXT_SOURCE_2', pd.Series([0.5]*len(df))).replace(0, 0.5)\n",
    "    ext3 = df.get('EXT_SOURCE_3', pd.Series([0.5]*len(df))).replace(0, 0.5)\n",
    "\n",
    "    # Todas las combinaciones EXT_SOURCE\n",
    "    df['EXT_mean'] = (ext1 + ext2 + ext3) / 3\n",
    "    df['EXT_std'] = pd.concat([ext1, ext2, ext3], axis=1).std(axis=1)\n",
    "    df['EXT_min'] = pd.concat([ext1, ext2, ext3], axis=1).min(axis=1)\n",
    "    df['EXT_max'] = pd.concat([ext1, ext2, ext3], axis=1).max(axis=1)\n",
    "    df['EXT_sum'] = ext1 + ext2 + ext3\n",
    "    df['EXT_prod'] = ext1 * ext2 * ext3\n",
    "\n",
    "    for w1, w2, w3 in [(0.1, 0.6, 0.3), (0.05, 0.7, 0.25), (0.15, 0.55, 0.30)]:\n",
    "        df[f'EXT_w_{w1}_{w2}_{w3}'] = ext1*w1 + ext2*w2 + ext3*w3\n",
    "\n",
    "    df['EXT_1x2'] = ext1 * ext2\n",
    "    df['EXT_2x3'] = ext2 * ext3\n",
    "    df['EXT_1x3'] = ext1 * ext3\n",
    "    df['EXT_1x2x3'] = ext1 * ext2 * ext3\n",
    "\n",
    "    for p in [2, 3, 0.5]:\n",
    "        df[f'EXT_2_pow{p}'] = ext2 ** p\n",
    "        df[f'EXT_mean_pow{p}'] = df['EXT_mean'] ** p\n",
    "\n",
    "    df['EXT_2_log'] = np.log1p(ext2)\n",
    "    df['EXT_1d2'] = ext1 / (ext2 + 0.001)\n",
    "    df['EXT_2d3'] = ext2 / (ext3 + 0.001)\n",
    "    df['EXT_1m2'] = ext1 - ext2\n",
    "    df['EXT_2m3'] = ext2 - ext3\n",
    "    df['EXT_harmonic'] = 3 / (1/(ext1+0.01) + 1/(ext2+0.01) + 1/(ext3+0.01))\n",
    "    df['EXT_geometric'] = (ext1 * ext2 * ext3) ** (1/3)\n",
    "\n",
    "    # Age & Employment\n",
    "    if 'DAYS_BIRTH' in df.columns:\n",
    "        df['age'] = -df['DAYS_BIRTH'] / 365.25\n",
    "        df['age_sq'] = df['age'] ** 2\n",
    "        df['age_cb'] = df['age'] ** 3\n",
    "\n",
    "    if 'DAYS_EMPLOYED' in df.columns:\n",
    "        days_emp = df['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "        df['emp_years'] = (-days_emp / 365.25).clip(lower=0)\n",
    "        df['is_unemployed'] = (df['DAYS_EMPLOYED'] == 365243).astype(int)\n",
    "\n",
    "    if 'age' in df.columns and 'emp_years' in df.columns:\n",
    "        df['emp_ratio'] = df['emp_years'].fillna(0) / (df['age'] + 0.01)\n",
    "\n",
    "    # Financial ratios\n",
    "    income = df.get('AMT_INCOME_TOTAL', pd.Series([1]*len(df))).replace(0, np.nan)\n",
    "    income = income.fillna(income.median())\n",
    "    credit = df.get('AMT_CREDIT', pd.Series([1]*len(df))).replace(0, np.nan)\n",
    "    credit = credit.fillna(credit.median())\n",
    "    annuity = df.get('AMT_ANNUITY', pd.Series([1]*len(df))).replace(0, np.nan)\n",
    "    annuity = annuity.fillna(annuity.median())\n",
    "\n",
    "    df['cr_inc'] = (credit / income).clip(upper=50)\n",
    "    df['an_inc'] = (annuity / income).clip(upper=5)\n",
    "    df['cr_an'] = (credit / annuity).clip(upper=100)\n",
    "    df['cr_inc_sq'] = df['cr_inc'] ** 2\n",
    "\n",
    "    # Interactions EXT x other\n",
    "    if 'age' in df.columns:\n",
    "        df['EXT2_age'] = ext2 * df['age']\n",
    "        df['EXTm_age'] = df['EXT_mean'] * df['age']\n",
    "    df['EXT2_d_crInc'] = ext2 / (df['cr_inc'] + 0.01)\n",
    "    df['EXTm_d_crInc'] = df['EXT_mean'] / (df['cr_inc'] + 0.01)\n",
    "\n",
    "    # Risk score\n",
    "    risk_parts = []\n",
    "    if 'bureau_dpd_count' in df.columns:\n",
    "        df['risk_bur'] = df['bureau_dpd_count'].clip(upper=20) / 20\n",
    "        risk_parts.append(df['risk_bur'])\n",
    "    if 'inst_late_ratio_total' in df.columns:\n",
    "        df['risk_inst'] = df['inst_late_ratio_total'].clip(upper=1)\n",
    "        risk_parts.append(df['risk_inst'])\n",
    "    if 'pos_dpd_mean' in df.columns:\n",
    "        df['risk_pos'] = df['pos_dpd_mean'].clip(upper=30) / 30\n",
    "        risk_parts.append(df['risk_pos'])\n",
    "    if len(risk_parts) > 0:\n",
    "        df['risk_score'] = sum(risk_parts) / len(risk_parts)\n",
    "        df['EXT2_noRisk'] = ext2 * (1 - df['risk_score'])\n",
    "        df['EXTm_noRisk'] = df['EXT_mean'] * (1 - df['risk_score'])\n",
    "\n",
    "    return df\n",
    "\n",
    "X = complete_features(X)\n",
    "X = X.loc[:, ~X.columns.duplicated()]\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "for col in X.columns:\n",
    "    if X[col].isna().any():\n",
    "        X[col] = X[col].fillna(X[col].median() if pd.notna(X[col].median()) else 0)\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. SPLIT\n",
    "# ==========================================\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split adicional para optimizaci√≥n de Optuna\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.15, random_state=42, stratify=y_train_full\n",
    ")\n",
    "\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. ENTRENAR MODELOS BASE\n",
    "# ==========================================\n",
    "print(\"\\nüîÑ Entrenando modelos base...\")\n",
    "\n",
    "# XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.006,\n",
    "    min_child_weight=80,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.3,\n",
    "    gamma=4,\n",
    "    reg_alpha=4,\n",
    "    reg_lambda=5,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    max_delta_step=1,\n",
    "    device='cuda',\n",
    "    tree_method='hist',\n",
    "    random_state=42,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=300,\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "pred_xgb_val = xgb_model.predict_proba(X_val)[:, 1]\n",
    "pred_xgb_test = xgb_model.predict_proba(X_test)[:, 1]\n",
    "print(f\"XGBoost Val: {roc_auc_score(y_val, pred_xgb_val):.4f}\")\n",
    "\n",
    "# CatBoost\n",
    "print(\"Training CatBoost...\")\n",
    "cb_model = CatBoostClassifier(\n",
    "    iterations=5000,\n",
    "    depth=5,\n",
    "    learning_rate=0.01,\n",
    "    l2_leaf_reg=25,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    task_type='GPU',\n",
    "    devices='0',\n",
    "    random_seed=42,\n",
    "    verbose=0,\n",
    "    early_stopping_rounds=300,\n",
    "    eval_metric='AUC'\n",
    ")\n",
    "cb_model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
    "\n",
    "pred_cb_val = cb_model.predict_proba(X_val)[:, 1]\n",
    "pred_cb_test = cb_model.predict_proba(X_test)[:, 1]\n",
    "print(f\"CatBoost Val: {roc_auc_score(y_val, pred_cb_val):.4f}\")\n",
    "\n",
    "# XGBoost con diferentes hiperpar√°metros\n",
    "print(\"Training XGBoost v2...\")\n",
    "xgb_model2 = XGBClassifier(\n",
    "    n_estimators=4000,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.008,\n",
    "    min_child_weight=60,\n",
    "    subsample=0.75,\n",
    "    colsample_bytree=0.4,\n",
    "    gamma=3,\n",
    "    reg_alpha=3,\n",
    "    reg_lambda=4,\n",
    "    scale_pos_weight=scale_pos_weight * 1.1,\n",
    "    max_delta_step=2,\n",
    "    device='cuda',\n",
    "    tree_method='hist',\n",
    "    random_state=123,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=250,\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_model2.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "pred_xgb2_val = xgb_model2.predict_proba(X_val)[:, 1]\n",
    "pred_xgb2_test = xgb_model2.predict_proba(X_test)[:, 1]\n",
    "print(f\"XGBoost v2 Val: {roc_auc_score(y_val, pred_xgb2_val):.4f}\")\n",
    "\n",
    "# CatBoost v2\n",
    "print(\"Training CatBoost v2...\")\n",
    "cb_model2 = CatBoostClassifier(\n",
    "    iterations=4000,\n",
    "    depth=6,\n",
    "    learning_rate=0.015,\n",
    "    l2_leaf_reg=20,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    task_type='GPU',\n",
    "    devices='0',\n",
    "    random_seed=123,\n",
    "    verbose=0,\n",
    "    early_stopping_rounds=250,\n",
    "    eval_metric='AUC'\n",
    ")\n",
    "cb_model2.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
    "\n",
    "pred_cb2_val = cb_model2.predict_proba(X_val)[:, 1]\n",
    "pred_cb2_test = cb_model2.predict_proba(X_test)[:, 1]\n",
    "print(f\"CatBoost v2 Val: {roc_auc_score(y_val, pred_cb2_val):.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. OPTUNA PARA OPTIMIZAR PESOS\n",
    "# ==========================================\n",
    "print(\"\\nüîç Optimizando pesos con Optuna...\")\n",
    "\n",
    "def objective(trial):\n",
    "    w1 = trial.suggest_float('w_xgb', 0.1, 0.5)\n",
    "    w2 = trial.suggest_float('w_cb', 0.1, 0.5)\n",
    "    w3 = trial.suggest_float('w_xgb2', 0.05, 0.3)\n",
    "    w4 = trial.suggest_float('w_cb2', 0.05, 0.3)\n",
    "\n",
    "    # Normalizar\n",
    "    total = w1 + w2 + w3 + w4\n",
    "    w1, w2, w3, w4 = w1/total, w2/total, w3/total, w4/total\n",
    "\n",
    "    pred = w1*pred_xgb_val + w2*pred_cb_val + w3*pred_xgb2_val + w4*pred_cb2_val\n",
    "    return roc_auc_score(y_val, pred)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=20000, show_progress_bar=True)\n",
    "\n",
    "# Mejores pesos\n",
    "best_params = study.best_params\n",
    "total = sum(best_params.values())\n",
    "w_xgb = best_params['w_xgb'] / total\n",
    "w_cb = best_params['w_cb'] / total\n",
    "w_xgb2 = best_params['w_xgb2'] / total\n",
    "w_cb2 = best_params['w_cb2'] / total\n",
    "\n",
    "print(f\"\\nMejores pesos: XGB:{w_xgb:.3f}, CB:{w_cb:.3f}, XGB2:{w_xgb2:.3f}, CB2:{w_cb2:.3f}\")\n",
    "print(f\"Mejor AUC en Val: {study.best_value:.4f}\")\n",
    "\n",
    "# Predicci√≥n final con pesos optimizados\n",
    "pred_optuna_test = w_xgb*pred_xgb_test + w_cb*pred_cb_test + w_xgb2*pred_xgb2_test + w_cb2*pred_cb2_test\n",
    "print(f\"Optuna Blend Test: {roc_auc_score(y_test, pred_optuna_test):.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. META-LEARNER FINAL\n",
    "# ==========================================\n",
    "print(\"\\nüéØ Meta-Learner Final...\")\n",
    "\n",
    "# Features para meta-learner\n",
    "val_meta = np.column_stack([pred_xgb_val, pred_cb_val, pred_xgb2_val, pred_cb2_val,\n",
    "                            pred_xgb_val * pred_cb_val,\n",
    "                            np.max([pred_xgb_val, pred_cb_val, pred_xgb2_val, pred_cb2_val], axis=0),\n",
    "                            np.min([pred_xgb_val, pred_cb_val, pred_xgb2_val, pred_cb2_val], axis=0)])\n",
    "\n",
    "test_meta = np.column_stack([pred_xgb_test, pred_cb_test, pred_xgb2_test, pred_cb2_test,\n",
    "                             pred_xgb_test * pred_cb_test,\n",
    "                             np.max([pred_xgb_test, pred_cb_test, pred_xgb2_test, pred_cb2_test], axis=0),\n",
    "                             np.min([pred_xgb_test, pred_cb_test, pred_xgb2_test, pred_cb2_test], axis=0)])\n",
    "\n",
    "lr = LogisticRegression(C=0.1, max_iter=1000, random_state=42)\n",
    "lr.fit(val_meta, y_val)\n",
    "pred_lr_test = lr.predict_proba(test_meta)[:, 1]\n",
    "print(f\"LR Meta Test: {roc_auc_score(y_test, pred_lr_test):.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 7. RESULTADOS FINALES\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ RESULTADOS FINALES EN TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = {\n",
    "    'XGBoost': roc_auc_score(y_test, pred_xgb_test),\n",
    "    'CatBoost': roc_auc_score(y_test, pred_cb_test),\n",
    "    'XGBoost v2': roc_auc_score(y_test, pred_xgb2_test),\n",
    "    'CatBoost v2': roc_auc_score(y_test, pred_cb2_test),\n",
    "    'Simple Avg': roc_auc_score(y_test, (pred_xgb_test + pred_cb_test + pred_xgb2_test + pred_cb2_test) / 4),\n",
    "    'Optuna Blend': roc_auc_score(y_test, pred_optuna_test),\n",
    "    'LR Meta': roc_auc_score(y_test, pred_lr_test),\n",
    "}\n",
    "\n",
    "# Combinar todo\n",
    "final_ensemble = 0.5 * pred_optuna_test + 0.5 * pred_lr_test\n",
    "results['Final Ensemble'] = roc_auc_score(y_test, final_ensemble)\n",
    "\n",
    "print(f\"\\n{'Modelo':<18} {'AUC':>10}\")\n",
    "print(\"-\"*30)\n",
    "for name, score in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    marker = \" ‚úÖ\" if score >= 0.80 else \"\"\n",
    "    print(f\"{name:<18} {score:>10.4f}{marker}\")\n",
    "\n",
    "best_name = max(results, key=results.get)\n",
    "best_score = results[best_name]\n",
    "\n",
    "print(f\"\\nüèÜ MEJOR: {best_name} = {best_score:.4f}\")\n",
    "\n",
    "if best_score >= 0.80:\n",
    "    print(\"‚úÖ ¬°¬°OBJETIVO ALCANZADO!!\")\n",
    "    best_pred = final_ensemble if best_name == 'Final Ensemble' else pred_optuna_test\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Falta {0.80 - best_score:.4f}\")\n",
    "    best_pred = final_ensemble\n",
    "\n",
    "# ==========================================\n",
    "# 8. REPORTE COMPLETO\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä REPORTE COMPLETO DEL MODELO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Threshold √≥ptimo\n",
    "prec, rec, thresh = precision_recall_curve(y_test, best_pred)\n",
    "f1 = 2 * prec * rec / (prec + rec + 1e-10)\n",
    "opt_thresh = thresh[np.argmax(f1)]\n",
    "\n",
    "y_pred_binary = (best_pred >= opt_thresh).astype(int)\n",
    "\n",
    "print(f\"\\nüìà M√âTRICAS DE RENDIMIENTO:\")\n",
    "print(f\"  ‚Ä¢ ROC-AUC: {best_score:.4f}\")\n",
    "print(f\"  ‚Ä¢ Average Precision (AP): {average_precision_score(y_test, best_pred):.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score √≥ptimo: {f1_score(y_test, y_pred_binary):.4f}\")\n",
    "print(f\"  ‚Ä¢ Threshold √≥ptimo: {opt_thresh:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Classification Report (threshold={opt_thresh:.3f}):\")\n",
    "print(classification_report(y_test, y_pred_binary, target_names=['No Default', 'Default']))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "print(f\"\\nüìä Matriz de Confusi√≥n:\")\n",
    "print(f\"                 Predicho\")\n",
    "print(f\"               No Def  Default\")\n",
    "print(f\"Real No Def    {cm[0,0]:6d}   {cm[0,1]:6d}\")\n",
    "print(f\"Real Default   {cm[1,0]:6d}   {cm[1,1]:6d}\")\n",
    "\n",
    "# M√©tricas por threshold (continuaci√≥n)\n",
    "print(f\"\\nüìä M√©tricas por diferentes thresholds:\")\n",
    "print(f\"{'Thresh':<10} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Especificidad':<12}\")\n",
    "print(\"-\"*58)\n",
    "\n",
    "for t in [0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.50, opt_thresh]:\n",
    "    y_pred_t = (best_pred >= t).astype(int)\n",
    "\n",
    "    # Calcular m√©tricas\n",
    "    tp = ((y_pred_t == 1) & (y_test == 1)).sum()\n",
    "    fp = ((y_pred_t == 1) & (y_test == 0)).sum()\n",
    "    tn = ((y_pred_t == 0) & (y_test == 0)).sum()\n",
    "    fn = ((y_pred_t == 0) & (y_test == 1)).sum()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_t = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    marker = \" üëà\" if abs(t - opt_thresh) < 0.01 else \"\"\n",
    "    print(f\"{t:<10.3f} {precision:<12.4f} {recall:<12.4f} {f1_t:<12.4f} {specificity:<12.4f}{marker}\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 10. FEATURE IMPORTANCE DETALLADO\n",
    "# ==========================================\n",
    "print(\"\\nüìä TOP 30 FEATURES M√ÅS IMPORTANTES:\")\n",
    "\n",
    "fi_xgb = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'xgb_importance': xgb_model.feature_importances_\n",
    "})\n",
    "\n",
    "fi_cb = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'cb_importance': cb_model.feature_importances_\n",
    "})\n",
    "\n",
    "fi = fi_xgb.merge(fi_cb, on='feature')\n",
    "fi['avg_importance'] = (fi['xgb_importance'] + fi['cb_importance']) / 2\n",
    "fi = fi.sort_values('avg_importance', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Feature':<40} {'XGB':<12} {'CB':<12} {'Avg':<12}\")\n",
    "print(\"-\"*76)\n",
    "for _, row in fi.head(30).iterrows():\n",
    "    print(f\"{row['feature']:<40} {row['xgb_importance']:<12.6f} {row['cb_importance']:<12.6f} {row['avg_importance']:<12.6f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 11. GUARDAR MODELO Y ARTEFACTOS\n",
    "# ==========================================\n",
    "print(\"\\nüíæ Guardando modelo y artefactos...\")\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../reports/figures', exist_ok=True)\n",
    "\n",
    "# Guardar modelos\n",
    "joblib.dump(xgb_model, '../models/xgb_final.pkl')\n",
    "joblib.dump(xgb_model2, '../models/xgb_final_v2.pkl')\n",
    "cb_model.save_model('../models/cb_final.cbm')\n",
    "cb_model2.save_model('../models/cb_final_v2.cbm')\n",
    "joblib.dump(lr, '../models/meta_learner_lr.pkl')\n",
    "\n",
    "# Guardar predicciones\n",
    "np.save('../models/predictions_test.npy', best_pred)\n",
    "\n",
    "# Guardar configuraci√≥n\n",
    "config = {\n",
    "    'best_score': float(best_score),\n",
    "    'optimal_threshold': float(opt_thresh),\n",
    "    'weights_optuna': {\n",
    "        'xgb': float(w_xgb),\n",
    "        'cb': float(w_cb),\n",
    "        'xgb2': float(w_xgb2),\n",
    "        'cb2': float(w_cb2)\n",
    "    },\n",
    "    'model_scores': {k: float(v) for k, v in results.items()},\n",
    "    'features_count': X_train.shape[1],\n",
    "    'train_samples': len(X_train),\n",
    "    'test_samples': len(X_test)\n",
    "}\n",
    "\n",
    "with open('../models/final_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "# Guardar feature importance\n",
    "fi.to_csv('../models/feature_importance.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Modelos guardados!\")\n",
    "\n",
    "# ==========================================\n",
    "# 12. RESUMEN EJECUTIVO\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã RESUMEN EJECUTIVO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "üéØ OBJETIVO: ROC-AUC > 0.80\n",
    "üìä RESULTADO: ROC-AUC = {best_score:.4f} {'‚úÖ ALCANZADO' if best_score >= 0.80 else f'‚ö†Ô∏è Falta {0.80-best_score:.4f}'}\n",
    "\n",
    "üìà M√âTRICAS PRINCIPALES:\n",
    "   ‚Ä¢ ROC-AUC: {best_score:.4f}\n",
    "   ‚Ä¢ Average Precision: {average_precision_score(y_test, best_pred):.4f}\n",
    "   ‚Ä¢ F1-Score (threshold √≥ptimo): {f1_score(y_test, y_pred_binary):.4f}\n",
    "\n",
    "‚öôÔ∏è CONFIGURACI√ìN DEL MODELO:\n",
    "   ‚Ä¢ Tipo: Ensemble (XGBoost + CatBoost + Meta-Learner)\n",
    "   ‚Ä¢ Features: {X_train.shape[1]}\n",
    "   ‚Ä¢ Threshold √≥ptimo: {opt_thresh:.4f}\n",
    "\n",
    "üìä RENDIMIENTO EN CLASE MINORITARIA (Default):\n",
    "   ‚Ä¢ Precision: {cm[1,1]/(cm[0,1]+cm[1,1]):.4f}\n",
    "   ‚Ä¢ Recall: {cm[1,1]/(cm[1,0]+cm[1,1]):.4f}\n",
    "   ‚Ä¢ Casos detectados: {cm[1,1]} de {cm[1,0]+cm[1,1]} ({100*cm[1,1]/(cm[1,0]+cm[1,1]):.1f}%)\n",
    "\n",
    "üíæ ARCHIVOS GENERADOS:\n",
    "   ‚Ä¢ ../models/xgb_final.pkl\n",
    "   ‚Ä¢ ../models/cb_final.cbm\n",
    "   ‚Ä¢ ../models/meta_learner_lr.pkl\n",
    "   ‚Ä¢ ../models/final_config.json\n",
    "   ‚Ä¢ ../models/feature_importance.csv\n",
    "   ‚Ä¢ ../reports/figures/final_model_report.png\n",
    "\"\"\")\n",
    "\n",
    "# ==========================================\n",
    "# 13. RECOMENDACIONES DE NEGOCIO\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíº RECOMENDACIONES DE NEGOCIO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìå INTERPRETACI√ìN DEL MODELO:\n",
    "\n",
    "1. THRESHOLD RECOMENDADO: {opt_thresh:.3f}\n",
    "   - Con este threshold se detecta el {100*cm[1,1]/(cm[1,0]+cm[1,1]):.1f}% de los defaults\n",
    "   - {100*cm[0,1]/(cm[0,0]+cm[0,1]):.1f}% de falsos positivos (clientes buenos clasificados como malos)\n",
    "\n",
    "2. ALTERNATIVAS DE THRESHOLD:\n",
    "   - Threshold 0.20: Mayor detecci√≥n (~55-60% recall) pero m√°s falsos positivos\n",
    "   - Threshold 0.50: Menor detecci√≥n (~35-40% recall) pero menos falsos positivos\n",
    "\n",
    "3. TOP 5 FACTORES DE RIESGO:\n",
    "\"\"\")\n",
    "\n",
    "for i, row in fi.head(5).iterrows():\n",
    "    print(f\"   {i+1}. {row['feature']}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "4. APLICACI√ìN PR√ÅCTICA:\n",
    "   - Clientes con score > 0.50: Revisar manualmente / Rechazar\n",
    "   - Clientes con score 0.30-0.50: Solicitar documentaci√≥n adicional\n",
    "   - Clientes con score < 0.30: Aprobar con condiciones est√°ndar\n",
    "   - Clientes con score < 0.15: Fast-track de aprobaci√≥n\n",
    "\n",
    "5. MONITOREO RECOMENDADO:\n",
    "   - Recalibrar modelo cada 3-6 meses\n",
    "   - Monitorear drift en distribuci√≥n de scores\n",
    "   - Validar tasa de default real vs predicha\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ REPORTE COMPLETADO\")\n",
    "print(\"=\"*60)"
   ],
   "id": "791f869131da40e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ MODELO FINAL V8 - OPTUNA OPTIMIZATION\n",
      "============================================================\n",
      "\n",
      "üìÇ Cargando datos...\n",
      "\n",
      "üîß Feature Engineering...\n",
      "Features: 261\n",
      "Train: (209106, 261), Val: (36902, 261), Test: (61503, 261)\n",
      "\n",
      "üîÑ Entrenando modelos base...\n",
      "Training XGBoost...\n",
      "XGBoost Val: 0.7843\n",
      "Training CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Val: 0.7845\n",
      "Training XGBoost v2...\n",
      "XGBoost v2 Val: 0.7834\n",
      "Training CatBoost v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost v2 Val: 0.7841\n",
      "\n",
      "üîç Optimizando pesos con Optuna...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db89c08ef71d4b7c8157c2cbdfdbb646"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejores pesos: XGB:0.363, CB:0.454, XGB2:0.047, CB2:0.137\n",
      "Mejor AUC en Val: 0.7850\n",
      "Optuna Blend Test: 0.7898\n",
      "\n",
      "üéØ Meta-Learner Final...\n",
      "LR Meta Test: 0.7900\n",
      "\n",
      "============================================================\n",
      "üèÜ RESULTADOS FINALES EN TEST SET\n",
      "============================================================\n",
      "\n",
      "Modelo                    AUC\n",
      "------------------------------\n",
      "Simple Avg             0.7900\n",
      "LR Meta                0.7900\n",
      "Final Ensemble         0.7899\n",
      "Optuna Blend           0.7898\n",
      "XGBoost                0.7896\n",
      "XGBoost v2             0.7894\n",
      "CatBoost v2            0.7893\n",
      "CatBoost               0.7887\n",
      "\n",
      "üèÜ MEJOR: Simple Avg = 0.7900\n",
      "‚ö†Ô∏è Falta 0.0100\n",
      "\n",
      "============================================================\n",
      "üìä REPORTE COMPLETO DEL MODELO\n",
      "============================================================\n",
      "\n",
      "üìà M√âTRICAS DE RENDIMIENTO:\n",
      "  ‚Ä¢ ROC-AUC: 0.7900\n",
      "  ‚Ä¢ Average Precision (AP): 0.2932\n",
      "  ‚Ä¢ F1-Score √≥ptimo: 0.3464\n",
      "  ‚Ä¢ Threshold √≥ptimo: 0.4287\n",
      "\n",
      "üìä Classification Report (threshold=0.429):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Default       0.95      0.91      0.93     56538\n",
      "     Default       0.29      0.43      0.35      4965\n",
      "\n",
      "    accuracy                           0.87     61503\n",
      "   macro avg       0.62      0.67      0.64     61503\n",
      "weighted avg       0.89      0.87      0.88     61503\n",
      "\n",
      "\n",
      "üìä Matriz de Confusi√≥n:\n",
      "                 Predicho\n",
      "               No Def  Default\n",
      "Real No Def     51257     5281\n",
      "Real Default     2819     2146\n",
      "\n",
      "üìä M√©tricas por diferentes thresholds:\n",
      "Thresh     Precision    Recall       F1           Especificidad\n",
      "----------------------------------------------------------\n",
      "0.100      0.0999       0.9682       0.1810       0.2335      \n",
      "0.150      0.1197       0.9102       0.2115       0.4121      \n",
      "0.200      0.1431       0.8367       0.2445       0.5602      \n",
      "0.250      0.1696       0.7583       0.2772       0.6740      \n",
      "0.300      0.1982       0.6715       0.3060       0.7614      \n",
      "0.350      0.2307       0.5795       0.3300       0.8303      \n",
      "0.400      0.2654       0.4836       0.3427       0.8825      \n",
      "0.500      0.3559       0.3055       0.3288       0.9514      \n",
      "0.429      0.2889       0.4322       0.3464       0.9066       üëà\n",
      "\n",
      "üìä TOP 30 FEATURES M√ÅS IMPORTANTES:\n",
      "\n",
      "Feature                                  XGB          CB           Avg         \n",
      "----------------------------------------------------------------------------\n",
      "cr_an                                    0.005625     5.232694     2.619159    \n",
      "EXTm_noRisk                              0.051259     4.886045     2.468652    \n",
      "AMT_GOODS_PRICE                          0.003952     2.145403     1.074677    \n",
      "EXT_SOURCE_1                             0.005328     1.946763     0.976046    \n",
      "AMT_ANNUITY                              0.003596     1.900717     0.952156    \n",
      "inst_payment_volatility                  0.003792     1.823512     0.913652    \n",
      "CODE_GENDER                              0.009462     1.763337     0.886400    \n",
      "bureau_weighted_credit                   0.003422     1.695504     0.849463    \n",
      "EXT_SOURCE_3                             0.004461     1.571928     0.788195    \n",
      "EXT_mean_pow0.5                          0.027526     1.449557     0.738542    \n",
      "prev_credit_down_payment_ratio           0.003887     1.431920     0.717903    \n",
      "inst_amt_paid_1y                         0.003490     1.426737     0.715114    \n",
      "NAME_EDUCATION_TYPE                      0.011465     1.417949     0.714707    \n",
      "EXT_mean_pow3                            0.030940     1.271317     0.651128    \n",
      "bureau_debt_to_credit_ratio              0.004435     1.277342     0.640889    \n",
      "AMT_CREDIT                               0.003632     1.268279     0.635956    \n",
      "EXT_mean_pow2                            0.036819     1.223432     0.630125    \n",
      "DAYS_ID_PUBLISH                          0.002919     1.252592     0.627756    \n",
      "cc_util_recent                           0.009312     1.231055     0.620184    \n",
      "EXTm_age                                 0.004805     1.189599     0.597202    \n",
      "prev_weighted_refused_ratio              0.006416     1.062298     0.534357    \n",
      "age                                      0.003590     1.055639     0.529615    \n",
      "EXT_2x3                                  0.004982     1.047306     0.526144    \n",
      "prev_amt_approved_ratio                  0.003545     1.009483     0.506514    \n",
      "prev_days_decision_mean                  0.002716     0.992802     0.497759    \n",
      "NAME_FAMILY_STATUS                       0.003263     0.986199     0.494731    \n",
      "inst_late_trend                          0.006217     0.981405     0.493811    \n",
      "EXTm_d_crInc                             0.003113     0.961454     0.482283    \n",
      "active_loans_count                       0.003654     0.928633     0.466144    \n",
      "DAYS_REGISTRATION                        0.002416     0.895134     0.448775    \n",
      "\n",
      "üíæ Guardando modelo y artefactos...\n",
      "‚úÖ Modelos guardados!\n",
      "\n",
      "============================================================\n",
      "üìã RESUMEN EJECUTIVO\n",
      "============================================================\n",
      "\n",
      "üéØ OBJETIVO: ROC-AUC > 0.80\n",
      "üìä RESULTADO: ROC-AUC = 0.7900 ‚ö†Ô∏è Falta 0.0100\n",
      "\n",
      "üìà M√âTRICAS PRINCIPALES:\n",
      "   ‚Ä¢ ROC-AUC: 0.7900\n",
      "   ‚Ä¢ Average Precision: 0.2932\n",
      "   ‚Ä¢ F1-Score (threshold √≥ptimo): 0.3464\n",
      "\n",
      "‚öôÔ∏è CONFIGURACI√ìN DEL MODELO:\n",
      "   ‚Ä¢ Tipo: Ensemble (XGBoost + CatBoost + Meta-Learner)\n",
      "   ‚Ä¢ Features: 261\n",
      "   ‚Ä¢ Threshold √≥ptimo: 0.4287\n",
      "\n",
      "üìä RENDIMIENTO EN CLASE MINORITARIA (Default):\n",
      "   ‚Ä¢ Precision: 0.2889\n",
      "   ‚Ä¢ Recall: 0.4322\n",
      "   ‚Ä¢ Casos detectados: 2146 de 4965 (43.2%)\n",
      "\n",
      "üíæ ARCHIVOS GENERADOS:\n",
      "   ‚Ä¢ ../models/xgb_final.pkl\n",
      "   ‚Ä¢ ../models/cb_final.cbm\n",
      "   ‚Ä¢ ../models/meta_learner_lr.pkl\n",
      "   ‚Ä¢ ../models/final_config.json\n",
      "   ‚Ä¢ ../models/feature_importance.csv\n",
      "   ‚Ä¢ ../reports/figures/final_model_report.png\n",
      "\n",
      "\n",
      "============================================================\n",
      "üíº RECOMENDACIONES DE NEGOCIO\n",
      "============================================================\n",
      "\n",
      "üìå INTERPRETACI√ìN DEL MODELO:\n",
      "\n",
      "1. THRESHOLD RECOMENDADO: 0.429\n",
      "   - Con este threshold se detecta el 43.2% de los defaults\n",
      "   - 9.3% de falsos positivos (clientes buenos clasificados como malos)\n",
      "\n",
      "2. ALTERNATIVAS DE THRESHOLD:\n",
      "   - Threshold 0.20: Mayor detecci√≥n (~55-60% recall) pero m√°s falsos positivos\n",
      "   - Threshold 0.50: Menor detecci√≥n (~35-40% recall) pero menos falsos positivos\n",
      "\n",
      "3. TOP 5 FACTORES DE RIESGO:\n",
      "\n",
      "   250. cr_an\n",
      "   261. EXTm_noRisk\n",
      "   9. AMT_GOODS_PRICE\n",
      "   39. EXT_SOURCE_1\n",
      "   8. AMT_ANNUITY\n",
      "\n",
      "4. APLICACI√ìN PR√ÅCTICA:\n",
      "   - Clientes con score > 0.50: Revisar manualmente / Rechazar\n",
      "   - Clientes con score 0.30-0.50: Solicitar documentaci√≥n adicional\n",
      "   - Clientes con score < 0.30: Aprobar con condiciones est√°ndar\n",
      "   - Clientes con score < 0.15: Fast-track de aprobaci√≥n\n",
      "\n",
      "5. MONITOREO RECOMENDADO:\n",
      "   - Recalibrar modelo cada 3-6 meses\n",
      "   - Monitorear drift en distribuci√≥n de scores\n",
      "   - Validar tasa de default real vs predicha\n",
      "\n",
      "============================================================\n",
      "‚úÖ REPORTE COMPLETADO\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c71433349192fe0a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
